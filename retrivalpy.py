# %%
import json
import re
import numpy as np
import faiss
from transformers import AutoTokenizer, AutoModel
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("./bge-large-zh-v1.5")
model = AutoModel.from_pretrained("./bge-large-zh-v1.5").to("cuda")
model.eval()

# %%


# %%
import torch
print(torch.__version__)
print(torch.version.cuda)
print(torch.cuda.is_available())


# %%
# === Step 1: åŠ è½½ OCR æ–‡æœ¬ ===
input_path = "ocr_results_filtered.jsonl"
with open(input_path, "r", encoding="utf-8") as f:
    pages = [json.loads(line.strip()) for line in f if line.strip()]

# === Step 2: åˆ‡åˆ†ä¸ºå¥å­çº§ chunk ===
sentence_chunks = []
for entry in pages:
    image = entry["image"]
    page = int(re.search(r"page_(\d+)", image).group(1))
    text = entry["cleaned_text"]
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)  # å¯æŒ‰éœ€è¦æ‰©å±•ä¸ºæ›´ä¸°å¯Œæ ‡ç‚¹åˆ†å¥
    for idx, sent in enumerate(sentences):
        sent = sent.strip()
        if len(sent) < 5:
            continue
        sentence_chunks.append({
            "text": sent,
            "page": page,
            "sentence_id": idx
        })

# %%

# === Step 3: ç”Ÿæˆå‘é‡ ===
@torch.no_grad()
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()

vectors = [get_embedding(c["text"]) for c in sentence_chunks]
vectors_np = np.vstack(vectors).astype("float32")

# === Step 4: æ„å»º FAISS æ£€ç´¢ç´¢å¼• ===
index = faiss.IndexFlatL2(vectors_np.shape[1])
index.add(vectors_np)

# %%

# === Step 5: æ¨¡æ‹Ÿæ£€ç´¢ï¼šè¿”å›å¥çª—ä¸Šä¸‹æ–‡ ===
def retrieve_with_context(query, top_k=5, window_size=2):
    q_vec = get_embedding(query).reshape(1, -1)
    D, I = index.search(q_vec, top_k)

    results = []
    for i in I[0]:
        center = i
        left = max(0, center - window_size)
        right = min(len(sentence_chunks), center + window_size + 1)
        window = [sentence_chunks[j]["text"] for j in range(left, right)]
        combined = "ã€‚".join(window) + "ã€‚"
        results.append({
            "context": combined,
            "page": sentence_chunks[center]["page"]
        })
    return results

# %%

# # === Step 6: æµ‹è¯• ===
# query = "ä½œè€…æ˜¯è°"
# res = retrieve_with_context(query)
# for r in res:
#     print(f"\nğŸ“„ é¡µç : {r['page']}\nğŸ” åŒ¹é…ä¸Šä¸‹æ–‡:\n{r['context']}")


# %%
# ä¿å­˜ FAISS å‘é‡ç´¢å¼•
faiss.write_index(index, "faiss_bge_big.index")

# ä¿å­˜å…ƒæ•°æ®ï¼ˆchunk å†…å®¹ + é¡µç ç­‰ï¼‰
with open("faiss_bge_metadata_big.json", "w", encoding="utf-8") as f:
    json.dump(sentence_chunks, f, ensure_ascii=False, indent=2)

print("âœ… å·²ä¿å­˜ FAISS ç´¢å¼•å’Œå…ƒæ•°æ®ï¼")

# %%
with open("faiss_bge_metadata_big.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

# åŠ è½½ faiss ç´¢å¼•
index = faiss.read_index("faiss_bge_big.index")

# å‘é‡è·å–å‡½æ•°
@torch.no_grad()
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    outputs = model.base_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()

# æ—¶é—´äº‹ä»¶æå–å‡½æ•°
def retrieve_events_by_year(year, top_k=20):
    query = f"åœ¨{year}å¹´å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ"
    q_vec = get_embedding(query).reshape(1, -1)
    D, I = index.search(q_vec, top_k)

    results = []
    for i in I[0]:
        meta = chunks[i]
        text = meta["text"]
        if re.search(rf"{year}", text):
          results.append(text)

    return "ã€‚".join(results)


# éå†æ—¶é—´èŒƒå›´ï¼Œæå–ä¿¡æ¯
year_summary = {}
for year in range(1930, 2008):
    context = retrieve_events_by_year(year)
    if context.strip():
        year_summary[str(year)] = context

# ä¿å­˜è¾“å‡º
with open("yearly_summary.json", "w", encoding="utf-8") as f:
    json.dump(year_summary, f, ensure_ascii=False, indent=2)




